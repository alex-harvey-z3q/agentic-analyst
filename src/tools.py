from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from pathlib import Path
from config import client, MODEL_NAME

# Embedding model used to convert text into vectors for similarity search.
EMBED_MODEL = "text-embedding-3-small"


def build_vectorstore(corpus_dir: str = "data/corpus") -> Chroma:
    """
    Build a Chroma vector store from all .txt files in the corpus directory.

    Steps:
    - Load all text files as Documents.
    - Split them into smaller overlapping chunks.
    - Embed those chunks using OpenAI embeddings.
    - Store them in a Chroma vector database for fast semantic search.
    """
    corpus_dir = Path(corpus_dir)

    docs = []

    for path in corpus_dir.glob("*.txt"):
        # TextLoader wraps a text file and knows how to load it as Documents.
        loader = TextLoader(str(path), encoding="utf-8")

        # loader.load() returns a list of Documents; we extend docs with them.
        docs.extend(loader.load())

    # Split long documents into overlapping chunks.
    # This helps retrieval: instead of retrieving whole files, we fetch
    # smaller, more precise pieces of text.
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,    # target size of each chunk in characters
        chunk_overlap=150, # how much chunks overlap with each other
    )
    split_docs = splitter.split_documents(docs)

    # Create an embeddings object that will call the OpenAI embeddings API
    # with the chosen embedding model.
    embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

    # Build a Chroma vector store from the split documents:
    # - Each chunk is embedded into a vector.
    # - The vectors + original text are stored in the "corpus" collection.
    vectordb = Chroma.from_documents(
        split_docs,
        embedding=embeddings,
        collection_name="corpus",
    )

    return vectordb


# Module-level cache for the vectorstore.
# This avoids rebuilding embeddings on every query.
_vectordb = None


def rag_search(query: str, k: int = 5) -> str:
    """
    Perform the 'retrieval' part of RAG.

    Given a query, this:
    - Lazily builds the vectorstore on first use.
    - Runs a semantic similarity search to find the top-k most relevant chunks.
    - Returns those chunks concatenated into a single context string.

    Note: This does NOT return the whole corpus; only the most relevant pieces.
    """
    global _vectordb

    # Build the vectorstore only once (lazy initialisation).
    if _vectordb is None:
        _vectordb = build_vectorstore()

    # Ask Chroma for the k most similar chunks to the query.
    docs = _vectordb.similarity_search(query, k=k)

    # Join the retrieved text chunks with blank lines between them.
    # This string is what we pass as "Context" to the LLM.
    return "\n\n".join([d.page_content for d in docs])


def call_llm(system_prompt: str, user_prompt: str) -> str:
    """
    Call the OpenAI chat model (Responses API) with:
    - a system prompt (role / behaviour instructions)
    - a user prompt (which includes the question + retrieved context)

    Returns the model's generated text as a plain string.
    """
    resp = client.responses.create(
        model=MODEL_NAME,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        max_output_tokens=1200,  # Upper bound on response length.
    )

    # Extract the text from the Responses API structure:
    # - output[0] : first candidate
    # - content[0]: first content item
    # - .text     : actual text string generated by the model
    msg = resp.output[0].content[0].text
    return msg
